#!/usr/bin/env python3
"""
Comparative Evaluation Script: FPN-RL vs Traditional PPRL

This script compares the new Federated Privacy-Preserving Neural Network 
Record Linkage (FPN-RL) mechanism with the traditional threshold-based 
PPRL approach used in PPRL.ipynb.

Author: AI Assistant for PACE-COMP3850-Group52
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
from typing import List, Tuple, Dict, Any
import warnings
warnings.filterwarnings('ignore')

# Import our new mechanism
from federated_embedding_linkage import FederatedEmbeddingLinkage, generate_sample_data_with_text

# Import traditional PPRL components
try:
    from PPRL import Link
    from BF import BF
except ImportError as e:
    print(f"Warning: Could not import traditional PPRL modules: {e}")
    print("Some comparisons may not be available.")


class ComparativeEvaluator:
    """
    Class to perform comprehensive comparison between FPN-RL and traditional PPRL.
    """
    
    def __init__(self):
        self.results = {
            'fpn_rl': {},
            'traditional_pprl': {},
            'comparison': {}
        }
    
    def load_test_data(self, use_provided_datasets: bool = False):
        """
        Load test datasets - either provided CSV files or generated samples.
        """
        if use_provided_datasets:
            try:
                # Try to load provided datasets
                alice_data = pd.read_csv('Alice_numrec_100_corr_25.csv')
                bob_data = pd.read_csv('Bob_numrec_100_corr_25.csv')
                
                # Create ground truth based on rec_id (first column)
                alice_ids = set(alice_data.iloc[:, 0])
                bob_ids = set(bob_data.iloc[:, 0])
                common_ids = alice_ids.intersection(bob_ids)
                
                ground_truth = []
                for common_id in common_ids:
                    alice_idx = alice_data[alice_data.iloc[:, 0] == common_id].index[0]
                    bob_idx = bob_data[bob_data.iloc[:, 0] == common_id].index[0]
                    ground_truth.append((alice_idx, bob_idx))
                
                print(f"Loaded provided datasets: {len(alice_data)} + {len(bob_data)} records")
                print(f"Ground truth matches: {len(ground_truth)}")
                
                return alice_data, bob_data, ground_truth
                
            except Exception as e:
                print(f"Could not load provided datasets: {e}")
                print("Falling back to generated sample data...")
        
        # Generate sample data with text
        print("Generating sample datasets with mixed structured/unstructured data...")
        data1, data2, ground_truth = generate_sample_data_with_text(
            n_records=200, match_rate=0.25
        )
        
        print(f"Generated datasets: {len(data1)} + {len(data2)} records")
        print(f"Ground truth matches: {len(ground_truth)}")
        print(f"Sample data1 columns: {list(data1.columns)}")
        print(f"Sample data2 columns: {list(data2.columns)}")
        
        return data1, data2, ground_truth
    
    def evaluate_fpn_rl(self, 
                        data1: pd.DataFrame, 
                        data2: pd.DataFrame, 
                        ground_truth: List[Tuple[int, int]],
                        text_col: str = None) -> Dict[str, Any]:
        """
        Evaluate the FPN-RL mechanism.
        """
        print("\n" + "="*50)
        print("EVALUATING FPN-RL MECHANISM")
        print("="*50)
        
        # Initialize FPN-RL model
        model = FederatedEmbeddingLinkage(
            embedding_dim=64,  # Smaller for faster training
            epsilon=1.0,
            delta=1e-5,
            min_sim_threshold=0.5
        )
        
        # Split data for training and testing
        train_size = int(0.7 * len(ground_truth))
        train_matches = ground_truth[:train_size]
        test_matches = ground_truth[train_size:]
        
        print(f"Training with {len(train_matches)} matches...")
        print(f"Testing with {len(test_matches)} matches...")
        
        # Train the model
        start_time = time.time()
        training_results = model.train(
            data1=data1,
            data2=data2,
            ground_truth_matches=train_matches,
            text_col=text_col,
            epochs=50,  # Reduced for faster evaluation
            batch_size=16
        )
        training_time = time.time() - start_time
        
        print(f"Training completed in {training_time:.2f} seconds")
        
        # Perform linkage on test data
        start_time = time.time()
        predicted_matches = model.link_records(data1, data2, text_col=text_col)
        linkage_time = time.time() - start_time
        
        print(f"Linkage completed in {linkage_time:.2f} seconds")
        print(f"Predicted {len(predicted_matches)} matches")
        
        # Evaluate quality
        precision, recall, f1 = model._evaluate_linkage_quality(
            predicted_matches, test_matches
        )
        
        # Get privacy information
        privacy_info = model.get_privacy_guarantees()
        
        results = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'training_time': training_time,
            'linkage_time': linkage_time,
            'predicted_matches': len(predicted_matches),
            'privacy_spent': privacy_info['epsilon_total'],
            'optimal_threshold': model.optimal_threshold,
            'privacy_guarantees': privacy_info
        }
        
        print(f"\nFPN-RL Results:")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"Privacy Budget Used: Îµ = {privacy_info['epsilon_total']:.4f}")
        print(f"Optimal Threshold: {model.optimal_threshold:.4f}")
        
        # Evaluate privacy-utility tradeoff
        print(f"\nEvaluating privacy-utility tradeoff...")
        epsilon_range = [0.1, 0.5, 1.0, 2.0, 5.0]
        tradeoff_results = model.evaluate_privacy_utility_tradeoff(
            epsilon_range, data1, data2, test_matches, text_col
        )
        results['tradeoff_analysis'] = tradeoff_results
        
        # Plot tradeoff
        model.plot_privacy_utility_tradeoff(
            tradeoff_results, 
            save_path='fpn_rl_privacy_utility_tradeoff.png'
        )
        
        return results
    
    def evaluate_traditional_pprl(self, 
                                 data1: pd.DataFrame, 
                                 data2: pd.DataFrame, 
                                 ground_truth: List[Tuple[int, int]]) -> Dict[str, Any]:
        """
        Evaluate the traditional PPRL mechanism.
        Note: This is a simplified evaluation using basic similarity matching
        since the full PPRL system requires specific data formats.
        """
        print("\n" + "="*50)
        print("EVALUATING TRADITIONAL PPRL BASELINE")
        print("="*50)
        
        # For comparison, we'll implement a simple threshold-based approach
        # using string similarity (mimicking the traditional PPRL approach)
        
        start_time = time.time()
        
        # Simple feature extraction (similar to traditional PPRL preprocessing)
        def extract_features(data):
            features = []
            for _, row in data.iterrows():
                # Convert all fields to strings and concatenate
                text_repr = ' '.join([str(val) for val in row.values if pd.notna(val)])
                features.append(text_repr)
            return features
        
        features1 = extract_features(data1)
        features2 = extract_features(data2)
        
        # Compute pairwise similarities (basic string matching)
        def string_similarity(s1, s2):
            """Simple string similarity measure."""
            import difflib
            return difflib.SequenceMatcher(None, s1.lower(), s2.lower()).ratio()
        
        # Find matches using threshold
        threshold = 0.8  # Traditional fixed threshold
        predicted_matches = []
        
        for i, feat1 in enumerate(features1):
            for j, feat2 in enumerate(features2):
                similarity = string_similarity(feat1, feat2)
                if similarity >= threshold:
                    predicted_matches.append((i, j, similarity))
        
        processing_time = time.time() - start_time
        
        # Evaluate quality
        if predicted_matches:
            predicted_set = {(i, j) for i, j, _ in predicted_matches}
            ground_truth_set = set(ground_truth)
            
            true_positives = len(predicted_set.intersection(ground_truth_set))
            false_positives = len(predicted_set - ground_truth_set)
            false_negatives = len(ground_truth_set - predicted_set)
            
            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        else:
            precision, recall, f1 = 0.0, 0.0, 0.0
        
        results = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'processing_time': processing_time,
            'predicted_matches': len(predicted_matches),
            'threshold': threshold,
            'privacy_guarantees': 'Basic string obfuscation (limited)'
        }
        
        print(f"\nTraditional PPRL Baseline Results:")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(f"Processing Time: {processing_time:.2f} seconds")
        print(f"Fixed Threshold: {threshold}")
        print(f"Predicted Matches: {len(predicted_matches)}")
        
        return results
    
    def compare_mechanisms(self, fpn_rl_results: Dict, traditional_results: Dict):
        """
        Compare the two mechanisms and generate comparison report.
        """
        print("\n" + "="*50)
        print("COMPARATIVE ANALYSIS")
        print("="*50)
        
        # Performance comparison
        print("\n1. LINKAGE QUALITY COMPARISON:")
        print(f"{'Metric':<15} {'FPN-RL':<10} {'Traditional':<12} {'Improvement':<12}")
        print("-" * 50)
        
        for metric in ['precision', 'recall', 'f1_score']:
            fpn_val = fpn_rl_results[metric]
            trad_val = traditional_results[metric]
            improvement = ((fpn_val - trad_val) / trad_val * 100) if trad_val > 0 else float('inf')
            
            print(f"{metric:<15} {fpn_val:<10.4f} {trad_val:<12.4f} {improvement:<12.2f}%")
        
        # Privacy comparison
        print("\n2. PRIVACY GUARANTEES COMPARISON:")
        print(f"FPN-RL: Differential Privacy with Îµ = {fpn_rl_results['privacy_spent']:.4f}")
        print(f"Traditional: {traditional_results['privacy_guarantees']}")
        
        # Computational efficiency
        print("\n3. COMPUTATIONAL EFFICIENCY:")
        print(f"FPN-RL Training Time: {fpn_rl_results['training_time']:.2f} seconds")
        print(f"FPN-RL Linkage Time: {fpn_rl_results['linkage_time']:.2f} seconds")
        print(f"Traditional Processing Time: {traditional_results['processing_time']:.2f} seconds")
        
        # Feature capabilities
        print("\n4. FEATURE CAPABILITIES:")
        features_comparison = {
            'Data Types': {'FPN-RL': 'Structured + Unstructured', 'Traditional': 'Structured only'},
            'Threshold Learning': {'FPN-RL': 'Automatic optimization', 'Traditional': 'Manual tuning'},
            'Privacy Mechanism': {'FPN-RL': 'Neural DP embeddings', 'Traditional': 'Bloom filters'},
            'Scalability': {'FPN-RL': 'Neural architecture', 'Traditional': 'Limited by blocking'},
            'Feature Learning': {'FPN-RL': 'Learned representations', 'Traditional': 'Hand-crafted'}
        }
        
        for feature, comparison in features_comparison.items():
            print(f"{feature}: FPN-RL ({comparison['FPN-RL']}) vs Traditional ({comparison['Traditional']})")
        
        # Generate comparison plots
        self.plot_comparison(fpn_rl_results, traditional_results)
        
        return {
            'performance_improvement': {
                'precision': fpn_rl_results['precision'] - traditional_results['precision'],
                'recall': fpn_rl_results['recall'] - traditional_results['recall'],
                'f1_score': fpn_rl_results['f1_score'] - traditional_results['f1_score']
            },
            'computational_overhead': fpn_rl_results['training_time'] + fpn_rl_results['linkage_time'] - traditional_results['processing_time'],
            'privacy_advantage': 'Formal DP guarantees vs basic obfuscation'
        }
    
    def plot_comparison(self, fpn_rl_results: Dict, traditional_results: Dict):
        """
        Generate comparison plots.
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Plot 1: Performance metrics comparison
        metrics = ['Precision', 'Recall', 'F1 Score']
        fpn_values = [fpn_rl_results['precision'], fpn_rl_results['recall'], fpn_rl_results['f1_score']]
        trad_values = [traditional_results['precision'], traditional_results['recall'], traditional_results['f1_score']]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax1.bar(x - width/2, fpn_values, width, label='FPN-RL', alpha=0.8, color='blue')
        ax1.bar(x + width/2, trad_values, width, label='Traditional PPRL', alpha=0.8, color='red')
        ax1.set_xlabel('Metrics')
        ax1.set_ylabel('Score')
        ax1.set_title('Linkage Quality Comparison')
        ax1.set_xticks(x)
        ax1.set_xticklabels(metrics)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Privacy-Utility Tradeoff (FPN-RL only)
        if 'tradeoff_analysis' in fpn_rl_results:
            tradeoff = fpn_rl_results['tradeoff_analysis']
            ax2.plot(tradeoff['epsilon'], tradeoff['f1_score'], 'b-o', label='F1 Score')
            ax2.set_xlabel('Privacy Budget (Îµ)')
            ax2.set_ylabel('F1 Score')
            ax2.set_title('FPN-RL: Privacy-Utility Tradeoff')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        # Plot 3: Computational time comparison
        methods = ['FPN-RL\n(Training)', 'FPN-RL\n(Linkage)', 'Traditional\n(Total)']
        times = [fpn_rl_results['training_time'], fpn_rl_results['linkage_time'], traditional_results['processing_time']]
        colors = ['lightblue', 'blue', 'red']
        
        ax3.bar(methods, times, color=colors, alpha=0.7)
        ax3.set_ylabel('Time (seconds)')
        ax3.set_title('Computational Time Comparison')
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: Number of matches found
        methods_matches = ['FPN-RL', 'Traditional PPRL']
        matches_found = [fpn_rl_results['predicted_matches'], traditional_results['predicted_matches']]
        
        ax4.bar(methods_matches, matches_found, color=['blue', 'red'], alpha=0.7)
        ax4.set_ylabel('Number of Matches Found')
        ax4.set_title('Matches Detection Comparison')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('comparative_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def run_comprehensive_evaluation(self, use_provided_datasets: bool = False):
        """
        Run the complete comparative evaluation.
        """
        print("Starting Comprehensive Evaluation of Privacy-Preserving Record Linkage Mechanisms")
        print("=" * 80)
        
        # Load test data
        data1, data2, ground_truth = self.load_test_data(use_provided_datasets)
        
        # Determine if we have text data for FPN-RL
        text_col = 'description' if 'description' in data1.columns else None
        
        # Evaluate FPN-RL
        self.results['fpn_rl'] = self.evaluate_fpn_rl(
            data1, data2, ground_truth, text_col
        )
        
        # Evaluate Traditional PPRL
        self.results['traditional_pprl'] = self.evaluate_traditional_pprl(
            data1, data2, ground_truth
        )
        
        # Compare mechanisms
        self.results['comparison'] = self.compare_mechanisms(
            self.results['fpn_rl'], 
            self.results['traditional_pprl']
        )
        
        # Generate summary report
        self.generate_summary_report()
        
        return self.results
    
    def generate_summary_report(self):
        """
        Generate a comprehensive summary report.
        """
        print("\n" + "="*80)
        print("FINAL EVALUATION SUMMARY")
        print("="*80)
        
        fpn_rl = self.results['fpn_rl']
        traditional = self.results['traditional_pprl']
        comparison = self.results['comparison']
        
        print(f"\nðŸŽ¯ OVERALL PERFORMANCE:")
        print(f"   FPN-RL achieved {fpn_rl['f1_score']:.4f} F1 score")
        print(f"   Traditional PPRL achieved {traditional['f1_score']:.4f} F1 score")
        print(f"   Performance improvement: {comparison['performance_improvement']['f1_score']:.4f}")
        
        print(f"\nðŸ” PRIVACY GUARANTEES:")
        print(f"   FPN-RL: Formal ({fpn_rl['privacy_spent']:.4f}, 1e-5)-differential privacy")
        print(f"   Traditional: {traditional['privacy_guarantees']}")
        
        print(f"\nâš¡ COMPUTATIONAL OVERHEAD:")
        print(f"   Additional time for FPN-RL: {comparison['computational_overhead']:.2f} seconds")
        print(f"   This includes model training which is a one-time cost")
        
        print(f"\nðŸš€ KEY ADVANTAGES OF FPN-RL:")
        print(f"   âœ“ Handles both structured and unstructured data")
        print(f"   âœ“ Automatic threshold optimization (learned: {fpn_rl['optimal_threshold']:.4f})")
        print(f"   âœ“ Formal differential privacy guarantees")
        print(f"   âœ“ Comprehensive privacy-utility tradeoff analysis")
        print(f"   âœ“ Neural feature learning capabilities")
        
        print(f"\nðŸ“Š RECOMMENDATION:")
        if fpn_rl['f1_score'] > traditional['f1_score']:
            print(f"   FPN-RL demonstrates superior performance with stronger privacy guarantees.")
            print(f"   Recommended for applications requiring high accuracy and formal privacy.")
        else:
            print(f"   Both mechanisms show comparable performance.")
            print(f"   Choice depends on specific privacy and computational requirements.")


def main():
    """
    Main evaluation function.
    """
    evaluator = ComparativeEvaluator()
    
    # Run evaluation with generated data (supports both structured and unstructured)
    print("Running evaluation with generated mixed-mode data...")
    results = evaluator.run_comprehensive_evaluation(use_provided_datasets=False)
    
    # Optionally, try with provided datasets if available
    try:
        print("\n" + "="*50)
        print("ATTEMPTING EVALUATION WITH PROVIDED DATASETS")
        print("="*50)
        evaluator_provided = ComparativeEvaluator()
        results_provided = evaluator_provided.run_comprehensive_evaluation(use_provided_datasets=True)
    except Exception as e:
        print(f"Could not evaluate with provided datasets: {e}")
        print("Evaluation completed with generated data only.")
    
    print("\nðŸŽ‰ Evaluation completed successfully!")
    print("Generated files:")
    print("   - fpn_rl_privacy_utility_tradeoff.png")
    print("   - comparative_analysis.png")
    print("   - federated_embedding_linkage.py (main implementation)")
    print("   - federated_embedding_linkage.md (documentation)")


if __name__ == "__main__":
    main()