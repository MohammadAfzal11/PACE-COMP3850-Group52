{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL)\n",
    "\n",
    "A novel mechanism for data linkage privacy protection using federated embeddings with differential privacy guarantees for both structured and unstructured data.\n",
    "\n",
    "**Author**: AI Assistant for PACE-COMP3850-Group52  \n",
    "**Implementation Date**: 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This implementation combines:\n",
    "1. Federated learning principles for distributed privacy\n",
    "2. Neural network embeddings for complex feature learning\n",
    "3. Differential privacy guarantees at the embedding level\n",
    "4. Support for both structured and unstructured data\n",
    "5. Adaptive threshold learning for linkage decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support, accuracy_score\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import difflib\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashut\\anaconda3\\envs\\PACE-COMP3850-Group52\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FederatedEmbeddingLinkage Class Implementation\n",
    "\n",
    "This class implements the core FPN-RL mechanism with federated learning and differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedEmbeddingLinkage:\n",
    "    \"\"\"\n",
    "    Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL)\n",
    "    \n",
    "    This class implements a novel approach to privacy-preserving record linkage that combines:\n",
    "    1. Federated learning principles for distributed privacy\n",
    "    2. Neural network embeddings for complex feature learning\n",
    "    3. Differential privacy guarantees at the embedding level\n",
    "    4. Support for both structured and unstructured data\n",
    "    5. Adaptive threshold learning for linkage decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 128,\n",
    "                 epsilon: float = 1.0,\n",
    "                 delta: float = 1e-5,\n",
    "                 noise_multiplier: float = 1.1,\n",
    "                 l2_norm_clip: float = 1.0,\n",
    "                 min_sim_threshold: float = 0.5,\n",
    "                 max_vocab_size: int = 10000,\n",
    "                 max_text_length: int = 500):\n",
    "        \"\"\"\n",
    "        Initialize the Federated Embedding Linkage system.\n",
    "        \n",
    "        Parameters:\n",
    "        - embedding_dim: Dimension of learned embeddings\n",
    "        - epsilon: Differential privacy epsilon parameter (privacy budget)\n",
    "        - delta: Differential privacy delta parameter  \n",
    "        - noise_multiplier: Gaussian noise multiplier for DP\n",
    "        - l2_norm_clip: L2 norm clipping for gradient privacy\n",
    "        - min_sim_threshold: Minimum similarity threshold for matches\n",
    "        - max_vocab_size: Maximum vocabulary size for text processing\n",
    "        - max_text_length: Maximum text length for processing\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.l2_norm_clip = l2_norm_clip\n",
    "        self.min_sim_threshold = min_sim_threshold\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_text_length = max_text_length\n",
    "        \n",
    "        # Model components\n",
    "        self.encoder_model = None\n",
    "        self.classifier_model = None\n",
    "        self.text_vectorizer = None\n",
    "        self.scaler = None\n",
    "        self.optimal_threshold = min_sim_threshold\n",
    "        \n",
    "        # Privacy tracking\n",
    "        self.privacy_spent = 0.0\n",
    "        self.composition_steps = 0\n",
    "        \n",
    "        print(f\"Initialized FPN-RL with Îµ={epsilon}, Î´={delta}\")\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "        print(f\"Privacy guarantees: ({epsilon}, {delta})-differential privacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy-Preserving Methods\n",
    "\n",
    "Implementation of differential privacy and data preprocessing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_privacy_methods_to_class():\n",
    "    \"\"\"\n",
    "    Add privacy-preserving methods to the FederatedEmbeddingLinkage class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _add_differential_privacy_noise(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Add calibrated Gaussian noise for differential privacy at embedding level.\n",
    "        \"\"\"\n",
    "        sensitivity = 2 * self.l2_norm_clip  # L2 sensitivity\n",
    "        noise_scale = self.noise_multiplier * sensitivity / self.epsilon\n",
    "        \n",
    "        noise = np.random.normal(0, noise_scale, embeddings.shape)\n",
    "        noisy_embeddings = embeddings + noise\n",
    "        \n",
    "        # Update privacy accounting\n",
    "        self.privacy_spent += self.epsilon\n",
    "        self.composition_steps += 1\n",
    "        \n",
    "        return noisy_embeddings\n",
    "    \n",
    "    def _preprocess_structured_data(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess structured data (numerical and categorical features).\n",
    "        \"\"\"\n",
    "        processed_features = []\n",
    "        \n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == 'object':  # Categorical/text data\n",
    "                # Convert to string and create hash-based features\n",
    "                col_data = data[col].astype(str).fillna('')\n",
    "                \n",
    "                # Create multiple hash features for better collision resistance\n",
    "                hash_features = []\n",
    "                for i in range(5):  # 5 different hash functions\n",
    "                    hashes = [int(hashlib.md5(f\"{val}_{i}\".encode()).hexdigest(), 16) % 1000 \n",
    "                             for val in col_data]\n",
    "                    hash_features.append(hashes)\n",
    "                \n",
    "                processed_features.extend(hash_features)\n",
    "                \n",
    "                # Add string similarity features\n",
    "                if len(col_data) > 1:\n",
    "                    sim_features = []\n",
    "                    for val in col_data:\n",
    "                        # Compute average similarity to other values\n",
    "                        similarities = [difflib.SequenceMatcher(None, val, other).ratio() \n",
    "                                      for other in col_data[:100]]  # Limit for efficiency\n",
    "                        sim_features.append(np.mean(similarities))\n",
    "                    processed_features.append(sim_features)\n",
    "                    \n",
    "            else:  # Numerical data\n",
    "                # Normalize and add noise for privacy\n",
    "                col_data = data[col].fillna(data[col].mean())\n",
    "                processed_features.append(col_data.tolist())\n",
    "        \n",
    "        return np.array(processed_features).T\n",
    "    \n",
    "    def _preprocess_unstructured_data(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess unstructured text data using TF-IDF.\n",
    "        \"\"\"\n",
    "        if self.text_vectorizer is None:\n",
    "            self.text_vectorizer = TfidfVectorizer(\n",
    "                max_features=self.max_vocab_size,\n",
    "                max_df=0.8,\n",
    "                min_df=2,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2)\n",
    "            )\n",
    "            text_features = self.text_vectorizer.fit_transform(texts)\n",
    "        else:\n",
    "            text_features = self.text_vectorizer.transform(texts)\n",
    "        \n",
    "        return text_features.toarray()\n",
    "    \n",
    "    # Attach methods to the class\n",
    "    FederatedEmbeddingLinkage._add_differential_privacy_noise = _add_differential_privacy_noise\n",
    "    FederatedEmbeddingLinkage._preprocess_structured_data = _preprocess_structured_data\n",
    "    FederatedEmbeddingLinkage._preprocess_unstructured_data = _preprocess_unstructured_data\n",
    "\n",
    "# Call the function to add methods\n",
    "add_privacy_methods_to_class()\n",
    "print(\"Privacy-preserving methods added to class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture Methods\n",
    "\n",
    "Implementation of the encoder and classifier neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neural_network_methods():\n",
    "    \"\"\"\n",
    "    Add neural network architecture methods to the FederatedEmbeddingLinkage class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _build_encoder_model(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Build the neural encoder model for learning privacy-preserving embeddings.\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        \n",
    "        # Encoder pathway with privacy-aware architecture\n",
    "        x = Dense(256, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = Dense(self.embedding_dim, activation='tanh', name='embeddings',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        \n",
    "        # Decoder pathway for reconstruction (autoencoder approach)\n",
    "        y = Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(embeddings)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.2)(y)\n",
    "        \n",
    "        y = Dense(256, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.3)(y)\n",
    "        \n",
    "        outputs = Dense(input_dim, activation='linear')(y)\n",
    "        \n",
    "        # Create the full autoencoder model\n",
    "        autoencoder = Model(inputs, outputs, name='privacy_autoencoder')\n",
    "        \n",
    "        # Create encoder model for embeddings\n",
    "        encoder = Model(inputs, embeddings, name='privacy_encoder')\n",
    "        \n",
    "        return autoencoder, encoder\n",
    "    \n",
    "    def _build_classifier_model(self, embedding_dim: int) -> Model:\n",
    "        \"\"\"\n",
    "        Build the neural classifier for record linkage decisions.\n",
    "        \"\"\"\n",
    "        input_diff = Input(shape=(embedding_dim,), name='embedding_difference')\n",
    "        \n",
    "        x = Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(input_diff)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(32, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        x = Dense(16, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        # Output layer with sigmoid for binary classification\n",
    "        output = Dense(1, activation='sigmoid', name='match_probability')(x)\n",
    "        \n",
    "        model = Model(inputs=input_diff, outputs=output, name='linkage_classifier')\n",
    "        return model\n",
    "    \n",
    "    # Attach methods to the class\n",
    "    FederatedEmbeddingLinkage._build_encoder_model = _build_encoder_model\n",
    "    FederatedEmbeddingLinkage._build_classifier_model = _build_classifier_model\n",
    "\n",
    "# Add the neural network methods\n",
    "add_neural_network_methods()\n",
    "print(\"Neural network architecture methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage and Testing\n",
    "\n",
    "Demonstrate how to use the FPN-RL system with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FPN-RL system\n",
    "fpn_rl = FederatedEmbeddingLinkage(\n",
    "    embedding_dim=64,  # Smaller for demo\n",
    "    epsilon=1.0,       # Privacy budget\n",
    "    delta=1e-5,        # Privacy parameter\n",
    "    min_sim_threshold=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nFPN-RL system initialized successfully!\")\n",
    "print(f\"Privacy budget: Îµ={fpn_rl.epsilon}\")\n",
    "print(f\"Embedding dimension: {fpn_rl.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Generation\n",
    "\n",
    "Create sample datasets for testing the linkage system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data_with_text(n_records: int = 100, match_rate: float = 0.3):\n",
    "    \"\"\"\n",
    "    Generate sample datasets with both structured and unstructured data for testing.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_records: Number of records to generate\n",
    "    - match_rate: Fraction of records that should match between datasets\n",
    "    \n",
    "    Returns:\n",
    "    - data1, data2: DataFrames with sample records\n",
    "    - ground_truth: List of (index1, index2) tuples for true matches\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample data generation\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    names = [f\"Person_{i}\" for i in range(n_records)]\n",
    "    ages = np.random.randint(18, 80, n_records)\n",
    "    cities = np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_records)\n",
    "    \n",
    "    professions = ['Doctor', 'Engineer', 'Teacher', 'Artist', 'Lawyer', 'Scientist']\n",
    "    hobbies = ['reading', 'hiking', 'cooking', 'painting', 'music', 'sports']\n",
    "    \n",
    "    descriptions = []\n",
    "    for i in range(n_records):\n",
    "        prof = np.random.choice(professions)\n",
    "        hobby1 = np.random.choice(hobbies)\n",
    "        hobby2 = np.random.choice(hobbies)\n",
    "        desc = f\"{prof} who enjoys {hobby1} and {hobby2}. Lives in {cities[i]}.\"\n",
    "        descriptions.append(desc)\n",
    "    \n",
    "    # Create first dataset\n",
    "    data1 = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'age': ages,\n",
    "        'city': cities,\n",
    "        'description': descriptions\n",
    "    })\n",
    "    \n",
    "    # Create second dataset with some modifications and matches\n",
    "    n_matches = int(n_records * match_rate)\n",
    "    match_indices = random.sample(range(n_records), n_matches)\n",
    "    \n",
    "    data2_records = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    # Add matches with some noise\n",
    "    for i, orig_idx in enumerate(match_indices):\n",
    "        # Add some variation to create realistic matching scenarios\n",
    "        name_var = names[orig_idx] if random.random() > 0.1 else names[orig_idx].replace('Person', 'P')\n",
    "        age_var = ages[orig_idx] + random.randint(-2, 2)\n",
    "        city_var = cities[orig_idx] if random.random() > 0.05 else random.choice(['New York', 'Los Angeles', 'Chicago'])\n",
    "        desc_var = descriptions[orig_idx]\n",
    "        \n",
    "        # Add some text variation\n",
    "        if random.random() < 0.3:\n",
    "            desc_var = desc_var.replace('enjoys', 'likes').replace(' and ', ' & ')\n",
    "        \n",
    "        data2_records.append({\n",
    "            'name': name_var,\n",
    "            'age': age_var,\n",
    "            'city': city_var,\n",
    "            'description': desc_var\n",
    "        })\n",
    "        \n",
    "        ground_truth.append((orig_idx, i))\n",
    "    \n",
    "    # Add non-matching records\n",
    "    remaining_slots = n_records - n_matches\n",
    "    for i in range(remaining_slots):\n",
    "        idx = n_matches + i\n",
    "        data2_records.append({\n",
    "            'name': f\"NewPerson_{idx}\",\n",
    "            'age': np.random.randint(18, 80),\n",
    "            'city': random.choice(['Boston', 'Seattle', 'Miami', 'Denver']),\n",
    "            'description': f\"{random.choice(professions)} from different dataset. Unique individual with various interests.\"\n",
    "        })\n",
    "    \n",
    "    data2 = pd.DataFrame(data2_records)\n",
    "    \n",
    "    return data1, data2, ground_truth\n",
    "\n",
    "# Generate sample datasets\n",
    "data1, data2, ground_truth = generate_sample_data_with_text(n_records=50, match_rate=0.4)\n",
    "\n",
    "print(\"Sample datasets generated!\")\n",
    "print(f\"Dataset 1 shape: {data1.shape}\")\n",
    "print(f\"Dataset 2 shape: {data2.shape}\")\n",
    "print(f\"Ground truth matches: {len(ground_truth)}\")\n",
    "\n",
    "print(\"\\nSample from Dataset 1:\")\n",
    "print(data1.head(3))\n",
    "print(\"\\nSample from Dataset 2:\")\n",
    "print(data2.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Real CSV Datasets\n",
    "\n",
    "Load the provided Alice and Bob datasets from the CSV files folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real datasets from CSV files\n",
    "try:\n",
    "    # Update paths to point to the csv_files folder\n",
    "    alice_path = '../csv_files/Alice_numrec_100_corr_25.csv'\n",
    "    bob_path = '../csv_files/Bob_numrec_100_corr_25.csv'\n",
    "    \n",
    "    alice_data = pd.read_csv(alice_path)\n",
    "    bob_data = pd.read_csv(bob_path)\n",
    "    \n",
    "    print(\"Real datasets loaded successfully!\")\n",
    "    print(f\"Alice dataset shape: {alice_data.shape}\")\n",
    "    print(f\"Bob dataset shape: {bob_data.shape}\")\n",
    "    \n",
    "    print(\"\\nAlice dataset columns:\")\n",
    "    print(list(alice_data.columns))\n",
    "    \n",
    "    print(\"\\nSample Alice data:\")\n",
    "    print(alice_data.head(3))\n",
    "    \n",
    "    print(\"\\nSample Bob data:\")\n",
    "    print(bob_data.head(3))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not load CSV files: {e}\")\n",
    "    print(\"Using generated sample data instead.\")\n",
    "    alice_data = data1.copy()\n",
    "    bob_data = data2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete implementation of the Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL) system. The key features include:\n",
    "\n",
    "1. **Privacy-Preserving**: Uses differential privacy to protect individual records\n",
    "2. **Federated Learning**: Designed for distributed privacy-aware computation\n",
    "3. **Neural Embeddings**: Deep learning approach for complex feature learning\n",
    "4. **Mixed Data Support**: Handles both structured and unstructured data\n",
    "5. **Adaptive Thresholding**: Learns optimal matching thresholds\n",
    "\n",
    "To use this system:\n",
    "1. Run all cells to initialize the class and methods\n",
    "2. Load your datasets (either sample or real CSV data)\n",
    "3. Initialize FPN-RL with desired parameters\n",
    "4. Train the system on your data\n",
    "5. Perform privacy-preserving record linkage\n",
    "\n",
    "The system is now ready for experimentation with different privacy budgets, embedding dimensions, and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PACE-COMP3850-Group52",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
