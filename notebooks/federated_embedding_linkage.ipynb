{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL)\n",
    "\n",
    "Neural network-based record linkage with federated learning and differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 1: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import difflib\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 2: FPN-RL Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedEmbeddingLinkage:\n",
    "    \"\"\"\n",
    "    Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL)\n",
    "    \n",
    "    This class implements a novel approach to privacy-preserving record linkage that combines:\n",
    "    1. Federated learning principles for distributed privacy\n",
    "    2. Neural network embeddings for complex feature learning\n",
    "    3. Differential privacy guarantees at the embedding level\n",
    "    4. Support for both structured and unstructured data\n",
    "    5. Adaptive threshold learning for linkage decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 128,\n",
    "                 epsilon: float = 1.0,\n",
    "                 delta: float = 1e-5,\n",
    "                 noise_multiplier: float = 1.1,\n",
    "                 l2_norm_clip: float = 1.0,\n",
    "                 min_sim_threshold: float = 0.5,\n",
    "                 max_vocab_size: int = 10000,\n",
    "                 max_text_length: int = 500):\n",
    "        \"\"\"\n",
    "        Initialize the Federated Embedding Linkage system.\n",
    "        \n",
    "        Parameters:\n",
    "        - embedding_dim: Dimension of learned embeddings\n",
    "        - epsilon: Differential privacy epsilon parameter (privacy budget)\n",
    "        - delta: Differential privacy delta parameter  \n",
    "        - noise_multiplier: Gaussian noise multiplier for DP\n",
    "        - l2_norm_clip: L2 norm clipping for gradient privacy\n",
    "        - min_sim_threshold: Minimum similarity threshold for matches\n",
    "        - max_vocab_size: Maximum vocabulary size for text processing\n",
    "        - max_text_length: Maximum text length for processing\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.l2_norm_clip = l2_norm_clip\n",
    "        self.min_sim_threshold = min_sim_threshold\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_text_length = max_text_length\n",
    "        \n",
    "        # Model components\n",
    "        self.encoder_model = None\n",
    "        self.classifier_model = None\n",
    "        self.text_vectorizer = None\n",
    "        self.scaler = None\n",
    "        self.optimal_threshold = min_sim_threshold\n",
    "        \n",
    "        # Privacy tracking\n",
    "        self.privacy_spent = 0.0\n",
    "        self.composition_steps = 0\n",
    "        \n",
    "        print(f\"Initialized FPN-RL with \u03b5={epsilon}, \u03b4={delta}\")\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "        print(f\"Privacy guarantees: ({epsilon}, {delta})-differential privacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 3: Privacy-Preserving Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_privacy_methods_to_class():\n",
    "    \"\"\"\n",
    "    Add privacy-preserving methods to the FederatedEmbeddingLinkage class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _add_differential_privacy_noise(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Add calibrated Gaussian noise for differential privacy at embedding level.\n",
    "        \"\"\"\n",
    "        sensitivity = 2 * self.l2_norm_clip  # L2 sensitivity\n",
    "        noise_scale = self.noise_multiplier * sensitivity / self.epsilon\n",
    "        \n",
    "        noise = np.random.normal(0, noise_scale, embeddings.shape)\n",
    "        noisy_embeddings = embeddings + noise\n",
    "        \n",
    "        # Update privacy accounting\n",
    "        self.privacy_spent += self.epsilon\n",
    "        self.composition_steps += 1\n",
    "        \n",
    "        return noisy_embeddings\n",
    "    \n",
    "    def _preprocess_structured_data(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess structured data (numerical and categorical features).\n",
    "        \"\"\"\n",
    "        processed_features = []\n",
    "        \n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == 'object':  # Categorical/text data\n",
    "                # Convert to string and create hash-based features\n",
    "                col_data = data[col].astype(str).fillna('')\n",
    "                \n",
    "                # Create multiple hash features for better collision resistance\n",
    "                hash_features = []\n",
    "                for i in range(5):  # 5 different hash functions\n",
    "                    hashes = [int(hashlib.md5(f\"{val}_{i}\".encode()).hexdigest(), 16) % 1000 \n",
    "                             for val in col_data]\n",
    "                    hash_features.append(hashes)\n",
    "                \n",
    "                processed_features.extend(hash_features)\n",
    "                \n",
    "                # Add string similarity features\n",
    "                if len(col_data) > 1:\n",
    "                    sim_features = []\n",
    "                    for val in col_data:\n",
    "                        # Compute average similarity to other values\n",
    "                        similarities = [difflib.SequenceMatcher(None, val, other).ratio() \n",
    "                                      for other in col_data[:100]]  # Limit for efficiency\n",
    "                        sim_features.append(np.mean(similarities))\n",
    "                    processed_features.append(sim_features)\n",
    "                    \n",
    "            else:  # Numerical data\n",
    "                # Normalize and add noise for privacy\n",
    "                col_data = data[col].fillna(data[col].mean())\n",
    "                processed_features.append(col_data.tolist())\n",
    "        \n",
    "        return np.array(processed_features).T\n",
    "    \n",
    "    def _preprocess_unstructured_data(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess unstructured text data using TF-IDF.\n",
    "        \"\"\"\n",
    "        if self.text_vectorizer is None:\n",
    "            self.text_vectorizer = TfidfVectorizer(\n",
    "                max_features=self.max_vocab_size,\n",
    "                max_df=0.8,\n",
    "                min_df=2,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2)\n",
    "            )\n",
    "            text_features = self.text_vectorizer.fit_transform(texts)\n",
    "        else:\n",
    "            text_features = self.text_vectorizer.transform(texts)\n",
    "        \n",
    "        return text_features.toarray()\n",
    "    \n",
    "    # Attach methods to the class\n",
    "    FederatedEmbeddingLinkage._add_differential_privacy_noise = _add_differential_privacy_noise\n",
    "    FederatedEmbeddingLinkage._preprocess_structured_data = _preprocess_structured_data\n",
    "    FederatedEmbeddingLinkage._preprocess_unstructured_data = _preprocess_unstructured_data\n",
    "\n",
    "# Call the function to add methods\n",
    "add_privacy_methods_to_class()\n",
    "print(\"Privacy-preserving methods added to class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 4: Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neural_network_methods():\n",
    "    \"\"\"\n",
    "    Add neural network architecture methods to the FederatedEmbeddingLinkage class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _build_encoder_model(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Build the neural encoder model for learning privacy-preserving embeddings.\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        \n",
    "        # Encoder pathway with privacy-aware architecture\n",
    "        x = Dense(256, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = Dense(self.embedding_dim, activation='tanh', name='embeddings',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        \n",
    "        # Decoder pathway for reconstruction (autoencoder approach)\n",
    "        y = Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(embeddings)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.2)(y)\n",
    "        \n",
    "        y = Dense(256, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.3)(y)\n",
    "        \n",
    "        outputs = Dense(input_dim, activation='linear')(y)\n",
    "        \n",
    "        # Create the full autoencoder model\n",
    "        autoencoder = Model(inputs, outputs, name='privacy_autoencoder')\n",
    "        \n",
    "        # Create encoder model for embeddings\n",
    "        encoder = Model(inputs, embeddings, name='privacy_encoder')\n",
    "        \n",
    "        return autoencoder, encoder\n",
    "    \n",
    "    def _build_classifier_model(self, embedding_dim: int) -> Model:\n",
    "        \"\"\"\n",
    "        Build the neural classifier for record linkage decisions.\n",
    "        \"\"\"\n",
    "        input_diff = Input(shape=(embedding_dim,), name='embedding_difference')\n",
    "        \n",
    "        x = Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(input_diff)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(32, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        x = Dense(16, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        # Output layer with sigmoid for binary classification\n",
    "        output = Dense(1, activation='sigmoid', name='match_probability')(x)\n",
    "        \n",
    "        model = Model(inputs=input_diff, outputs=output, name='linkage_classifier')\n",
    "        return model\n",
    "    \n",
    "    # Attach methods to the class\n",
    "    FederatedEmbeddingLinkage._build_encoder_model = _build_encoder_model\n",
    "    FederatedEmbeddingLinkage._build_classifier_model = _build_classifier_model\n",
    "\n",
    "# Add the neural network methods\n",
    "add_neural_network_methods()\n",
    "print(\"Neural network architecture methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 5: Record Linkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 6: Configure FPN-RL Parameters\n\n**Modify these values to tune the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THESE PARAMETERS TO TUNE LINKAGE ACCURACY:\n",
    "embedding_dim = 64          # Embedding dimension (higher = more capacity)\n",
    "epsilon = 1.0               # Privacy budget (lower = more privacy)\n",
    "delta = 1e-5                # Privacy parameter\n",
    "min_sim_threshold = 0.7     # Similarity threshold for matching (0-1)\n",
    "learning_rate = 0.001       # Learning rate for training\n",
    "epochs = 50                 # Number of training epochs\n",
    "batch_size = 32             # Batch size for training\n",
    "\n",
    "fpn_rl = FederatedEmbeddingLinkage(\n",
    "    embedding_dim=embedding_dim,\n",
    "    epsilon=epsilon,\n",
    "    delta=delta,\n",
    "    min_sim_threshold=min_sim_threshold\n",
    ")\n",
    "\n",
    "print(f\"FPN-RL initialized with \u03b5={epsilon}, embedding_dim={embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 7: Sample Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data_with_text(n_records: int = 100, match_rate: float = 0.3):\n",
    "    \"\"\"\n",
    "    Generate sample datasets with both structured and unstructured data for testing.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_records: Number of records to generate\n",
    "    - match_rate: Fraction of records that should match between datasets\n",
    "    \n",
    "    Returns:\n",
    "    - data1, data2: DataFrames with sample records\n",
    "    - ground_truth: List of (index1, index2) tuples for true matches\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample data generation\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    names = [f\"Person_{i}\" for i in range(n_records)]\n",
    "    ages = np.random.randint(18, 80, n_records)\n",
    "    cities = np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_records)\n",
    "    \n",
    "    professions = ['Doctor', 'Engineer', 'Teacher', 'Artist', 'Lawyer', 'Scientist']\n",
    "    hobbies = ['reading', 'hiking', 'cooking', 'painting', 'music', 'sports']\n",
    "    \n",
    "    descriptions = []\n",
    "    for i in range(n_records):\n",
    "        prof = np.random.choice(professions)\n",
    "        hobby1 = np.random.choice(hobbies)\n",
    "        hobby2 = np.random.choice(hobbies)\n",
    "        desc = f\"{prof} who enjoys {hobby1} and {hobby2}. Lives in {cities[i]}.\"\n",
    "        descriptions.append(desc)\n",
    "    \n",
    "    # Create first dataset\n",
    "    data1 = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'age': ages,\n",
    "        'city': cities,\n",
    "        'description': descriptions\n",
    "    })\n",
    "    \n",
    "    # Create second dataset with some modifications and matches\n",
    "    n_matches = int(n_records * match_rate)\n",
    "    match_indices = random.sample(range(n_records), n_matches)\n",
    "    \n",
    "    data2_records = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    # Add matches with some noise\n",
    "    for i, orig_idx in enumerate(match_indices):\n",
    "        # Add some variation to create realistic matching scenarios\n",
    "        name_var = names[orig_idx] if random.random() > 0.1 else names[orig_idx].replace('Person', 'P')\n",
    "        age_var = ages[orig_idx] + random.randint(-2, 2)\n",
    "        city_var = cities[orig_idx] if random.random() > 0.05 else random.choice(['New York', 'Los Angeles', 'Chicago'])\n",
    "        desc_var = descriptions[orig_idx]\n",
    "        \n",
    "        # Add some text variation\n",
    "        if random.random() < 0.3:\n",
    "            desc_var = desc_var.replace('enjoys', 'likes').replace(' and ', ' & ')\n",
    "        \n",
    "        data2_records.append({\n",
    "            'name': name_var,\n",
    "            'age': age_var,\n",
    "            'city': city_var,\n",
    "            'description': desc_var\n",
    "        })\n",
    "        \n",
    "        ground_truth.append((orig_idx, i))\n",
    "    \n",
    "    # Add non-matching records\n",
    "    remaining_slots = n_records - n_matches\n",
    "    for i in range(remaining_slots):\n",
    "        idx = n_matches + i\n",
    "        data2_records.append({\n",
    "            'name': f\"NewPerson_{idx}\",\n",
    "            'age': np.random.randint(18, 80),\n",
    "            'city': random.choice(['Boston', 'Seattle', 'Miami', 'Denver']),\n",
    "            'description': f\"{random.choice(professions)} from different dataset. Unique individual with various interests.\"\n",
    "        })\n",
    "    \n",
    "    data2 = pd.DataFrame(data2_records)\n",
    "    \n",
    "    return data1, data2, ground_truth\n",
    "\n",
    "# Generate sample datasets\n",
    "data1, data2, ground_truth = generate_sample_data_with_text(n_records=50, match_rate=0.4)\n",
    "\n",
    "print(\"Sample datasets generated!\")\n",
    "print(f\"Dataset 1 shape: {data1.shape}\")\n",
    "print(f\"Dataset 2 shape: {data2.shape}\")\n",
    "print(f\"Ground truth matches: {len(ground_truth)}\")\n",
    "\n",
    "print(\"\\nSample from Dataset 1:\")\n",
    "print(data1.head(3))\n",
    "print(\"\\nSample from Dataset 2:\")\n",
    "print(data2.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 8: Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, data2, ground_truth = generate_sample_data_with_text(n_records=50, match_rate=0.4)\n",
    "print(f\"Generated {len(data1)} records in dataset 1\")\n",
    "print(f\"Generated {len(data2)} records in dataset 2\")\n",
    "print(f\"Ground truth matches: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 9: Load CSV Datasets\n\n**Modify dataset paths to test different data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THESE PATHS TO USE DIFFERENT DATASETS:\n",
    "csv_dataset1_path = '../csv_files/Alice_numrec_100_corr_50.csv'\n",
    "csv_dataset2_path = '../csv_files/Bob_numrec_100_corr_50.csv'\n",
    "\n",
    "try:\n",
    "    csv_data1 = pd.read_csv(csv_dataset1_path)\n",
    "    csv_data2 = pd.read_csv(csv_dataset2_path)\n",
    "    print(f\"Loaded CSV data: {len(csv_data1)} and {len(csv_data2)} records\")\n",
    "    print(f\"Columns: {csv_data1.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    csv_data1, csv_data2 = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 10: Train FPN-RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = fpn_rl.train_and_link(\n",
    "    data1, data2, ground_truth,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall: {results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 11: Parameter Comparison Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def test_fpn_rl_parameter_variations(param_name, param_values, base_params, datasets_dict):\n",
    "    results = []\n",
    "    \n",
    "    for dataset_name, (path1, path2) in datasets_dict.items():\n",
    "        try:\n",
    "            d1 = pd.read_csv(path1)\n",
    "            d2 = pd.read_csv(path2)\n",
    "            \n",
    "            d1['full_name'] = d1['first_name'] + ' ' + d1['last_name']\n",
    "            d2['full_name'] = d2['first_name'] + ' ' + d2['last_name']\n",
    "            \n",
    "            gt = [(i, i) for i in range(min(len(d1), len(d2)))]\n",
    "            \n",
    "            for param_val in param_values:\n",
    "                params = base_params.copy()\n",
    "                params[param_name] = param_val\n",
    "                \n",
    "                model = FederatedEmbeddingLinkage(\n",
    "                    embedding_dim=params['embedding_dim'],\n",
    "                    epsilon=params['epsilon'],\n",
    "                    delta=params['delta'],\n",
    "                    min_sim_threshold=params['min_sim_threshold']\n",
    "                )\n",
    "                \n",
    "                res = model.train_and_link(\n",
    "                    d1, d2, gt,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size']\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'param_value': param_val,\n",
    "                    'precision': res['precision'],\n",
    "                    'recall': res['recall'],\n",
    "                    'f1_score': res['f1_score']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with dataset {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "base_params_fpn = {\n",
    "    'embedding_dim': 64,\n",
    "    'epsilon': 1.0,\n",
    "    'delta': 1e-5,\n",
    "    'min_sim_threshold': 0.7,\n",
    "    'epochs': 30,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "datasets_fpn = {\n",
    "    '100_corr_25': ('../csv_files/Alice_numrec_100_corr_25.csv', '../csv_files/Bob_numrec_100_corr_25.csv'),\n",
    "    '100_corr_50': ('../csv_files/Alice_numrec_100_corr_50.csv', '../csv_files/Bob_numrec_100_corr_50.csv'),\n",
    "    '500_corr_25': ('../csv_files/Alice_numrec_500_corr_25.csv', '../csv_files/Bob_numrec_500_corr_25.csv'),\n",
    "    '500_corr_50': ('../csv_files/Alice_numrec_500_corr_50.csv', '../csv_files/Bob_numrec_500_corr_50.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 12: Privacy Budget (Epsilon) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values_fpn = [0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "epsilon_results_fpn = test_fpn_rl_parameter_variations('epsilon', epsilon_values_fpn, base_params_fpn, datasets_fpn)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for dataset_name in datasets_fpn.keys():\n",
    "    data = epsilon_results_fpn[epsilon_results_fpn['dataset'] == dataset_name]\n",
    "    if len(data) > 0:\n",
    "        axes[0].plot(data['param_value'], data['precision'], marker='o', label=dataset_name)\n",
    "        axes[1].plot(data['param_value'], data['recall'], marker='o', label=dataset_name)\n",
    "        axes[2].plot(data['param_value'], data['f1_score'], marker='o', label=dataset_name)\n",
    "\n",
    "axes[0].set_xlabel('Privacy Budget (\u03b5)')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('FPN-RL: Precision vs Privacy Budget')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Privacy Budget (\u03b5)')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('FPN-RL: Recall vs Privacy Budget')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Privacy Budget (\u03b5)')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('FPN-RL: F1 Score vs Privacy Budget')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fpn_rl_epsilon_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nEpsilon Results:')\n",
    "print(epsilon_results_fpn.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 13: Embedding Dimension Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_values = [32, 64, 128, 256]\n",
    "embedding_dim_results = test_fpn_rl_parameter_variations('embedding_dim', embedding_dim_values, base_params_fpn, datasets_fpn)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for dataset_name in datasets_fpn.keys():\n",
    "    data = embedding_dim_results[embedding_dim_results['dataset'] == dataset_name]\n",
    "    if len(data) > 0:\n",
    "        axes[0].plot(data['param_value'], data['precision'], marker='s', label=dataset_name)\n",
    "        axes[1].plot(data['param_value'], data['recall'], marker='s', label=dataset_name)\n",
    "        axes[2].plot(data['param_value'], data['f1_score'], marker='s', label=dataset_name)\n",
    "\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('FPN-RL: Precision vs Embedding Dimension')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Embedding Dimension')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('FPN-RL: Recall vs Embedding Dimension')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Embedding Dimension')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('FPN-RL: F1 Score vs Embedding Dimension')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fpn_rl_embedding_dim_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nEmbedding Dimension Results:')\n",
    "print(embedding_dim_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 14: Similarity Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values_fpn = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "threshold_results_fpn = test_fpn_rl_parameter_variations('min_sim_threshold', threshold_values_fpn, base_params_fpn, datasets_fpn)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for dataset_name in datasets_fpn.keys():\n",
    "    data = threshold_results_fpn[threshold_results_fpn['dataset'] == dataset_name]\n",
    "    if len(data) > 0:\n",
    "        axes[0].plot(data['param_value'], data['precision'], marker='^', label=dataset_name)\n",
    "        axes[1].plot(data['param_value'], data['recall'], marker='^', label=dataset_name)\n",
    "        axes[2].plot(data['param_value'], data['f1_score'], marker='^', label=dataset_name)\n",
    "\n",
    "axes[0].set_xlabel('Similarity Threshold')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('FPN-RL: Precision vs Similarity Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Similarity Threshold')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('FPN-RL: Recall vs Similarity Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Similarity Threshold')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('FPN-RL: F1 Score vs Similarity Threshold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fpn_rl_threshold_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nThreshold Results:')\n",
    "print(threshold_results_fpn.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE 15: Combined Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for dataset_name in datasets_fpn.keys():\n",
    "    eps_data = epsilon_results_fpn[epsilon_results_fpn['dataset'] == dataset_name]\n",
    "    if len(eps_data) > 0:\n",
    "        axes[0, 0].plot(eps_data['param_value'], eps_data['f1_score'], marker='o', label=dataset_name)\n",
    "    \n",
    "    emb_data = embedding_dim_results[embedding_dim_results['dataset'] == dataset_name]\n",
    "    if len(emb_data) > 0:\n",
    "        axes[0, 1].plot(emb_data['param_value'], emb_data['f1_score'], marker='s', label=dataset_name)\n",
    "    \n",
    "    th_data = threshold_results_fpn[threshold_results_fpn['dataset'] == dataset_name]\n",
    "    if len(th_data) > 0:\n",
    "        axes[1, 0].plot(th_data['param_value'], th_data['f1_score'], marker='^', label=dataset_name)\n",
    "\n",
    "axes[0, 0].set_title('F1 Score vs Privacy Budget (\u03b5)')\n",
    "axes[0, 0].set_xlabel('Privacy Budget (\u03b5)')\n",
    "axes[0, 0].set_ylabel('F1 Score')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('F1 Score vs Embedding Dimension')\n",
    "axes[0, 1].set_xlabel('Embedding Dimension')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_title('F1 Score vs Similarity Threshold')\n",
    "axes[1, 0].set_xlabel('Similarity Threshold')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fpn_rl_combined_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}