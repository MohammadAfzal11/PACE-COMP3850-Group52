{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL)\n",
    "\n",
    "A novel mechanism for data linkage privacy protection using federated embeddings with differential privacy guarantees for both structured and unstructured data.\n",
    "\n",
    "**Author**: AI Assistant for PACE-COMP3850-Group52  \n",
    "**Implementation Date**: 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This implementation combines:\n",
    "1. Federated learning principles for distributed privacy\n",
    "2. Neural network embeddings for complex feature learning\n",
    "3. Differential privacy guarantees at the embedding level\n",
    "4. Support for both structured and unstructured data\n",
    "5. Adaptive threshold learning for linkage decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import difflib\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FederatedEmbeddingLinkage Class Implementation\n",
    "\n",
    "This class implements the core FPN-RL mechanism with federated learning and differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedEmbeddingLinkage:\n",
    "    \"\"\"\n",
    "    Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL)\n",
    "    \n",
    "    This class implements a novel approach to privacy-preserving record linkage that combines:\n",
    "    1. Federated learning principles for distributed privacy\n",
    "    2. Neural network embeddings for complex feature learning\n",
    "    3. Differential privacy guarantees at the embedding level\n",
    "    4. Support for both structured and unstructured data\n",
    "    5. Adaptive threshold learning for linkage decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 128,\n",
    "                 epsilon: float = 1.0,\n",
    "                 delta: float = 1e-5,\n",
    "                 noise_multiplier: float = 1.1,\n",
    "                 l2_norm_clip: float = 1.0,\n",
    "                 min_sim_threshold: float = 0.5,\n",
    "                 max_vocab_size: int = 10000,\n",
    "                 max_text_length: int = 500):\n",
    "        \"\"\"\n",
    "        Initialize the Federated Embedding Linkage system.\n",
    "        \n",
    "        Parameters:\n",
    "        - embedding_dim: Dimension of learned embeddings\n",
    "        - epsilon: Differential privacy epsilon parameter (privacy budget)\n",
    "        - delta: Differential privacy delta parameter  \n",
    "        - noise_multiplier: Gaussian noise multiplier for DP\n",
    "        - l2_norm_clip: L2 norm clipping for gradient privacy\n",
    "        - min_sim_threshold: Minimum similarity threshold for matches\n",
    "        - max_vocab_size: Maximum vocabulary size for text processing\n",
    "        - max_text_length: Maximum text length for processing\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.l2_norm_clip = l2_norm_clip\n",
    "        self.min_sim_threshold = min_sim_threshold\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_text_length = max_text_length\n",
    "        \n",
    "        # Model components\n",
    "        self.encoder_model = None\n",
    "        self.classifier_model = None\n",
    "        self.text_vectorizer = None\n",
    "        self.scaler = None\n",
    "        self.optimal_threshold = min_sim_threshold\n",
    "        \n",
    "        # Privacy tracking\n",
    "        self.privacy_spent = 0.0\n",
    "        self.composition_steps = 0\n",
    "        \n",
    "        print(f\"Initialized FPN-RL with ε={epsilon}, δ={delta}\")\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "        print(f\"Privacy guarantees: ({epsilon}, {delta})-differential privacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy-Preserving Methods\n",
    "\n",
    "Implementation of differential privacy and data preprocessing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privacy-preserving methods added to class!\n"
     ]
    }
   ],
   "source": [
    "def add_privacy_methods_to_class():\n",
    "    \"\"\"\n",
    "    Add privacy-preserving methods to the FederatedEmbeddingLinkage class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _add_differential_privacy_noise(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Add calibrated Gaussian noise for differential privacy at embedding level.\n",
    "        \"\"\"\n",
    "        sensitivity = 2 * self.l2_norm_clip  # L2 sensitivity\n",
    "        noise_scale = self.noise_multiplier * sensitivity / self.epsilon\n",
    "        \n",
    "        noise = np.random.normal(0, noise_scale, embeddings.shape)\n",
    "        noisy_embeddings = embeddings + noise\n",
    "        \n",
    "        # Update privacy accounting\n",
    "        self.privacy_spent += self.epsilon\n",
    "        self.composition_steps += 1\n",
    "        \n",
    "        return noisy_embeddings\n",
    "    \n",
    "    def _preprocess_structured_data(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess structured data (numerical and categorical features).\n",
    "        \"\"\"\n",
    "        processed_features = []\n",
    "        \n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == 'object':  # Categorical/text data\n",
    "                # Convert to string and create hash-based features\n",
    "                col_data = data[col].astype(str).fillna('')\n",
    "                \n",
    "                # Create multiple hash features for better collision resistance\n",
    "                hash_features = []\n",
    "                for i in range(5):  # 5 different hash functions\n",
    "                    hashes = [int(hashlib.md5(f\"{val}_{i}\".encode()).hexdigest(), 16) % 1000 \n",
    "                             for val in col_data]\n",
    "                    hash_features.append(hashes)\n",
    "                \n",
    "                processed_features.extend(hash_features)\n",
    "                \n",
    "                # Add string similarity features\n",
    "                if len(col_data) > 1:\n",
    "                    sim_features = []\n",
    "                    for val in col_data:\n",
    "                        # Compute average similarity to other values\n",
    "                        similarities = [difflib.SequenceMatcher(None, val, other).ratio() \n",
    "                                      for other in col_data[:100]]  # Limit for efficiency\n",
    "                        sim_features.append(np.mean(similarities))\n",
    "                    processed_features.append(sim_features)\n",
    "                    \n",
    "            else:  # Numerical data\n",
    "                # Normalize and add noise for privacy\n",
    "                col_data = data[col].fillna(data[col].mean())\n",
    "                processed_features.append(col_data.tolist())\n",
    "        \n",
    "        return np.array(processed_features).T\n",
    "    \n",
    "    def _preprocess_unstructured_data(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess unstructured text data using TF-IDF.\n",
    "        \"\"\"\n",
    "        if self.text_vectorizer is None:\n",
    "            self.text_vectorizer = TfidfVectorizer(\n",
    "                max_features=self.max_vocab_size,\n",
    "                max_df=0.8,\n",
    "                min_df=2,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2)\n",
    "            )\n",
    "            text_features = self.text_vectorizer.fit_transform(texts)\n",
    "        else:\n",
    "            text_features = self.text_vectorizer.transform(texts)\n",
    "        \n",
    "        return text_features.toarray()\n",
    "    \n",
    "    # Attach methods to the class\n",
    "    FederatedEmbeddingLinkage._add_differential_privacy_noise = _add_differential_privacy_noise\n",
    "    FederatedEmbeddingLinkage._preprocess_structured_data = _preprocess_structured_data\n",
    "    FederatedEmbeddingLinkage._preprocess_unstructured_data = _preprocess_unstructured_data\n",
    "\n",
    "# Call the function to add methods\n",
    "add_privacy_methods_to_class()\n",
    "print(\"Privacy-preserving methods added to class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture Methods\n",
    "\n",
    "Implementation of the encoder and classifier neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network architecture methods added!\n"
     ]
    }
   ],
   "source": [
    "def add_neural_network_methods():\n",
    "    \"\"\"\n",
    "    Add neural network architecture methods to the FederatedEmbeddingLinkage class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _build_encoder_model(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Build the neural encoder model for learning privacy-preserving embeddings.\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        \n",
    "        # Encoder pathway with privacy-aware architecture\n",
    "        x = Dense(256, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embeddings = Dense(self.embedding_dim, activation='tanh', name='embeddings',\n",
    "                          kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        \n",
    "        # Decoder pathway for reconstruction (autoencoder approach)\n",
    "        y = Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(embeddings)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.2)(y)\n",
    "        \n",
    "        y = Dense(256, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.3)(y)\n",
    "        \n",
    "        outputs = Dense(input_dim, activation='linear')(y)\n",
    "        \n",
    "        # Create the full autoencoder model\n",
    "        autoencoder = Model(inputs, outputs, name='privacy_autoencoder')\n",
    "        \n",
    "        # Create encoder model for embeddings\n",
    "        encoder = Model(inputs, embeddings, name='privacy_encoder')\n",
    "        \n",
    "        return autoencoder, encoder\n",
    "    \n",
    "    def _build_classifier_model(self, embedding_dim: int) -> Model:\n",
    "        \"\"\"\n",
    "        Build the neural classifier for record linkage decisions.\n",
    "        \"\"\"\n",
    "        input_diff = Input(shape=(embedding_dim,), name='embedding_difference')\n",
    "        \n",
    "        x = Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(input_diff)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(32, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        x = Dense(16, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        # Output layer with sigmoid for binary classification\n",
    "        output = Dense(1, activation='sigmoid', name='match_probability')(x)\n",
    "        \n",
    "        model = Model(inputs=input_diff, outputs=output, name='linkage_classifier')\n",
    "        return model\n",
    "    \n",
    "    # Attach methods to the class\n",
    "    FederatedEmbeddingLinkage._build_encoder_model = _build_encoder_model\n",
    "    FederatedEmbeddingLinkage._build_classifier_model = _build_classifier_model\n",
    "\n",
    "# Add the neural network methods\n",
    "add_neural_network_methods()\n",
    "print(\"Neural network architecture methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage and Testing\n",
    "\n",
    "Demonstrate how to use the FPN-RL system with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized FPN-RL with ε=1.0, δ=1e-05\n",
      "Embedding dimension: 64\n",
      "Privacy guarantees: (1.0, 1e-05)-differential privacy\n",
      "\n",
      "FPN-RL system initialized successfully!\n",
      "Privacy budget: ε=1.0\n",
      "Embedding dimension: 64\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FPN-RL system\n",
    "fpn_rl = FederatedEmbeddingLinkage(\n",
    "    embedding_dim=64,  # Smaller for demo\n",
    "    epsilon=1.0,       # Privacy budget\n",
    "    delta=1e-5,        # Privacy parameter\n",
    "    min_sim_threshold=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nFPN-RL system initialized successfully!\")\n",
    "print(f\"Privacy budget: ε={fpn_rl.epsilon}\")\n",
    "print(f\"Embedding dimension: {fpn_rl.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Generation\n",
    "\n",
    "Create sample datasets for testing the linkage system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datasets generated!\n",
      "Dataset 1 shape: (50, 4)\n",
      "Dataset 2 shape: (50, 4)\n",
      "Ground truth matches: 20\n",
      "\n",
      "Sample from Dataset 1:\n",
      "       name  age      city                                        description\n",
      "0  Person_0   56   Houston  Doctor who enjoys music and music. Lives in Ho...\n",
      "1  Person_1   69   Houston  Doctor who enjoys reading and reading. Lives i...\n",
      "2  Person_2   46  New York  Doctor who enjoys painting and cooking. Lives ...\n",
      "\n",
      "Sample from Dataset 2:\n",
      "        name  age      city                                        description\n",
      "0  Person_40   61   Phoenix  Engineer who enjoys painting and sports. Lives...\n",
      "1   Person_7   36  New York  Artist who likes hiking & reading. Lives in Ne...\n",
      "2   Person_1   69   Houston  Doctor who enjoys reading and reading. Lives i...\n"
     ]
    }
   ],
   "source": [
    "def generate_sample_data_with_text(n_records: int = 100, match_rate: float = 0.3):\n",
    "    \"\"\"\n",
    "    Generate sample datasets with both structured and unstructured data for testing.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_records: Number of records to generate\n",
    "    - match_rate: Fraction of records that should match between datasets\n",
    "    \n",
    "    Returns:\n",
    "    - data1, data2: DataFrames with sample records\n",
    "    - ground_truth: List of (index1, index2) tuples for true matches\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample data generation\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    names = [f\"Person_{i}\" for i in range(n_records)]\n",
    "    ages = np.random.randint(18, 80, n_records)\n",
    "    cities = np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_records)\n",
    "    \n",
    "    professions = ['Doctor', 'Engineer', 'Teacher', 'Artist', 'Lawyer', 'Scientist']\n",
    "    hobbies = ['reading', 'hiking', 'cooking', 'painting', 'music', 'sports']\n",
    "    \n",
    "    descriptions = []\n",
    "    for i in range(n_records):\n",
    "        prof = np.random.choice(professions)\n",
    "        hobby1 = np.random.choice(hobbies)\n",
    "        hobby2 = np.random.choice(hobbies)\n",
    "        desc = f\"{prof} who enjoys {hobby1} and {hobby2}. Lives in {cities[i]}.\"\n",
    "        descriptions.append(desc)\n",
    "    \n",
    "    # Create first dataset\n",
    "    data1 = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'age': ages,\n",
    "        'city': cities,\n",
    "        'description': descriptions\n",
    "    })\n",
    "    \n",
    "    # Create second dataset with some modifications and matches\n",
    "    n_matches = int(n_records * match_rate)\n",
    "    match_indices = random.sample(range(n_records), n_matches)\n",
    "    \n",
    "    data2_records = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    # Add matches with some noise\n",
    "    for i, orig_idx in enumerate(match_indices):\n",
    "        # Add some variation to create realistic matching scenarios\n",
    "        name_var = names[orig_idx] if random.random() > 0.1 else names[orig_idx].replace('Person', 'P')\n",
    "        age_var = ages[orig_idx] + random.randint(-2, 2)\n",
    "        city_var = cities[orig_idx] if random.random() > 0.05 else random.choice(['New York', 'Los Angeles', 'Chicago'])\n",
    "        desc_var = descriptions[orig_idx]\n",
    "        \n",
    "        # Add some text variation\n",
    "        if random.random() < 0.3:\n",
    "            desc_var = desc_var.replace('enjoys', 'likes').replace(' and ', ' & ')\n",
    "        \n",
    "        data2_records.append({\n",
    "            'name': name_var,\n",
    "            'age': age_var,\n",
    "            'city': city_var,\n",
    "            'description': desc_var\n",
    "        })\n",
    "        \n",
    "        ground_truth.append((orig_idx, i))\n",
    "    \n",
    "    # Add non-matching records\n",
    "    remaining_slots = n_records - n_matches\n",
    "    for i in range(remaining_slots):\n",
    "        idx = n_matches + i\n",
    "        data2_records.append({\n",
    "            'name': f\"NewPerson_{idx}\",\n",
    "            'age': np.random.randint(18, 80),\n",
    "            'city': random.choice(['Boston', 'Seattle', 'Miami', 'Denver']),\n",
    "            'description': f\"{random.choice(professions)} from different dataset. Unique individual with various interests.\"\n",
    "        })\n",
    "    \n",
    "    data2 = pd.DataFrame(data2_records)\n",
    "    \n",
    "    return data1, data2, ground_truth\n",
    "\n",
    "# Generate sample datasets\n",
    "data1, data2, ground_truth = generate_sample_data_with_text(n_records=50, match_rate=0.4)\n",
    "\n",
    "print(\"Sample datasets generated!\")\n",
    "print(f\"Dataset 1 shape: {data1.shape}\")\n",
    "print(f\"Dataset 2 shape: {data2.shape}\")\n",
    "print(f\"Ground truth matches: {len(ground_truth)}\")\n",
    "\n",
    "print(\"\\nSample from Dataset 1:\")\n",
    "print(data1.head(3))\n",
    "print(\"\\nSample from Dataset 2:\")\n",
    "print(data2.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Real CSV Datasets\n",
    "\n",
    "Load the provided Alice and Bob datasets from the CSV files folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real datasets loaded successfully!\n",
      "Alice dataset shape: (100, 19)\n",
      "Bob dataset shape: (100, 5)\n",
      "\n",
      "Alice dataset columns:\n",
      "['rec_id', 'voter_reg_num', 'name_prefix', 'first_name', 'middle_name', 'last_name', 'name_suffix', 'age', 'gender', 'race', 'ethnic', 'street_address', 'city', 'state', 'zip_code', 'full_phone_num', 'birth_place', 'register_date', 'download_month']\n",
      "\n",
      "Sample Alice data:\n",
      "    rec_id voter_reg_num name_prefix  first_name  middle_name  last_name  \\\n",
      "0  5653067     sharonica    thompson  high point        27265        NaN   \n",
      "1  2335653          jean        wall     clayton        27520        NaN   \n",
      "2  5358028         danya        wise    st pauls        28384        NaN   \n",
      "\n",
      "   name_suffix  age  gender  race  ethnic  street_address  city  state  \\\n",
      "0          NaN  NaN     NaN   NaN     NaN             NaN   NaN    NaN   \n",
      "1          NaN  NaN     NaN   NaN     NaN             NaN   NaN    NaN   \n",
      "2          NaN  NaN     NaN   NaN     NaN             NaN   NaN    NaN   \n",
      "\n",
      "   zip_code  full_phone_num  birth_place  register_date  download_month  \n",
      "0       NaN             NaN          NaN            NaN             NaN  \n",
      "1       NaN             NaN          NaN            NaN             NaN  \n",
      "2       NaN             NaN          NaN            NaN             NaN  \n",
      "\n",
      "Sample Bob data:\n",
      "     recid givenname     surname        suburb postcode\n",
      "0  4381862      eric  throneburg  morrisvillze    27260\n",
      "1  7062391   barbara     dresdow        cantom    287l6\n",
      "2  4745036   rinabew     kodhani   morrisville    27260\n"
     ]
    }
   ],
   "source": [
    "# Load real datasets from CSV files\n",
    "try:\n",
    "    # Update paths to point to the csv_files folder\n",
    "    alice_path = '../csv_files/Alice_numrec_100_corr_25.csv'\n",
    "    bob_path = '../csv_files/Bob_numrec_100_corr_25.csv'\n",
    "    \n",
    "    alice_data = pd.read_csv(alice_path)\n",
    "    bob_data = pd.read_csv(bob_path)\n",
    "    \n",
    "    print(\"Real datasets loaded successfully!\")\n",
    "    print(f\"Alice dataset shape: {alice_data.shape}\")\n",
    "    print(f\"Bob dataset shape: {bob_data.shape}\")\n",
    "    \n",
    "    print(\"\\nAlice dataset columns:\")\n",
    "    print(list(alice_data.columns))\n",
    "    \n",
    "    print(\"\\nSample Alice data:\")\n",
    "    print(alice_data.head(3))\n",
    "    \n",
    "    print(\"\\nSample Bob data:\")\n",
    "    print(bob_data.head(3))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not load CSV files: {e}\")\n",
    "    print(\"Using generated sample data instead.\")\n",
    "    alice_data = data1.copy()\n",
    "    bob_data = data2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete implementation of the Federated Privacy-Preserving Neural Network Record Linkage (FPN-RL) system. The key features include:\n",
    "\n",
    "1. **Privacy-Preserving**: Uses differential privacy to protect individual records\n",
    "2. **Federated Learning**: Designed for distributed privacy-aware computation\n",
    "3. **Neural Embeddings**: Deep learning approach for complex feature learning\n",
    "4. **Mixed Data Support**: Handles both structured and unstructured data\n",
    "5. **Adaptive Thresholding**: Learns optimal matching thresholds\n",
    "\n",
    "To use this system:\n",
    "1. Run all cells to initialize the class and methods\n",
    "2. Load your datasets (either sample or real CSV data)\n",
    "3. Initialize FPN-RL with desired parameters\n",
    "4. Train the system on your data\n",
    "5. Perform privacy-preserving record linkage\n",
    "\n",
    "The system is now ready for experimentation with different privacy budgets, embedding dimensions, and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
